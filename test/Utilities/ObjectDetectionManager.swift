//
//  ObjectDetectionManager.swift
//  MemoryLink
//
//  å®æ—¶ç‰©ä½“æ£€æµ‹ï¼ˆä½¿ç”¨ Vision Frameworkï¼‰
//

import Foundation
import AVFoundation
import Vision
import UIKit

// MARK: - Detected Object Model
struct DetectedObject: Identifiable {
    let id = UUID()
    let label: String          // ç‰©ä½“åç§° (e.g., "cup", "chair")
    let confidence: Float      // ç½®ä¿¡åº¦ (0.0 - 1.0)
    let boundingBox: CGRect    // è¾¹ç•Œæ¡†ä½ç½®
    let emoji: String          // å¯¹åº”çš„ emoji
    
    // ç‰©ä½“åç§°æ˜ å°„åˆ° emoji
    static func emojiForLabel(_ label: String) -> String {
        let emojiMap: [String: String] = [
            "cup": "â˜•ï¸",
            "mug": "â˜•ï¸",
            "bottle": "ğŸ¾",
            "glass": "ğŸ¥¤",
            "book": "ğŸ“š",
            "notebook": "ğŸ““",
            "chair": "ğŸª‘",
            "couch": "ğŸ›‹",
            "table": "ğŸª‘",
            "laptop": "ğŸ’»",
            "phone": "ğŸ“±",
            "camera": "ğŸ“·",
            "clock": "â°",
            "watch": "âŒšï¸",
            "vase": "ğŸº",
            "plant": "ğŸª´",
            "picture": "ğŸ–¼",
            "frame": "ğŸ–¼",
            "lamp": "ğŸ’¡",
            "keyboard": "âŒ¨ï¸",
            "mouse": "ğŸ–±",
            "bag": "ğŸ‘œ",
            "backpack": "ğŸ’",
            "shoe": "ğŸ‘Ÿ",
            "hat": "ğŸ©",
            "glasses": "ğŸ‘“",
            "pen": "ğŸ–Š",
            "pencil": "âœï¸"
        ]
        
        return emojiMap[label.lowercased()] ?? "ğŸ“¦"
    }
}

// MARK: - Object Detection Manager
class ObjectDetectionManager: NSObject, ObservableObject {
    
    @Published var detectedObjects: [DetectedObject] = []
    @Published var isDetecting = false
    
    private var captureSession: AVCaptureSession?
    private var videoOutput: AVCaptureVideoDataOutput?
    private let outputQueue = DispatchQueue(label: "com.memorylink.objectdetection")
    
    // Vision request - ä½¿ç”¨å›¾åƒåˆ†ç±»
    private lazy var classificationRequest: VNClassifyImageRequest = {
        let request = VNClassifyImageRequest { [weak self] request, error in
            self?.processClassification(request: request, error: error)
        }
        return request
    }()
    
    // MARK: - Setup
    func setupCamera() -> AVCaptureSession? {
        // å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œç›´æ¥è¿”å›
        if let existingSession = captureSession {
            return existingSession
        }
        
        let session = AVCaptureSession()
        session.sessionPreset = .high
        
        guard let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back) else {
            print("Failed to get camera device")
            return nil
        }
        
        guard let input = try? AVCaptureDeviceInput(device: device) else {
            print("Failed to create camera input")
            return nil
        }
        
        if session.canAddInput(input) {
            session.addInput(input)
        } else {
            print("Cannot add input to session")
            return nil
        }
        
        let output = AVCaptureVideoDataOutput()
        output.setSampleBufferDelegate(self, queue: outputQueue)
        output.alwaysDiscardsLateVideoFrames = true
        output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        
        if session.canAddOutput(output) {
            session.addOutput(output)
        } else {
            print("Cannot add output to session")
            return nil
        }
        
        self.captureSession = session
        self.videoOutput = output
        
        print("Camera setup completed successfully")
        
        return session
    }
    
    func startDetection() {
        guard let session = captureSession else { return }
        
        isDetecting = true
        
        DispatchQueue.global(qos: .userInitiated).async {
            session.startRunning()
            print("Camera session started")
        }
    }
    
    func stopDetection() {
        isDetecting = false
        
        DispatchQueue.global(qos: .userInitiated).async {
            self.captureSession?.stopRunning()
            print("Camera session stopped")
        }
    }
    
    // MARK: - Process Classification
    private func processClassification(request: VNRequest, error: Error?) {
        guard let results = request.results as? [VNClassificationObservation] else {
            return
        }
        
        // è·å–æœ€é«˜ç½®ä¿¡åº¦çš„ç»“æœï¼ˆé™ä½é˜ˆå€¼è®©æ›´å®¹æ˜“æ£€æµ‹ï¼‰
        let topResults = results.filter { $0.confidence > 0.15 }.prefix(5)
        
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            
            self.detectedObjects = topResults.enumerated().map { index, observation in
                // ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“åˆ›å»ºè¾¹ç•Œæ¡†
                let spacing: CGFloat = 0.28
                let y = CGFloat(index) * spacing + 0.15
                
                // æ¸…ç†æ ‡ç­¾åç§°
                let cleanLabel = self.cleanLabel(observation.identifier)
                
                return DetectedObject(
                    label: cleanLabel,
                    confidence: observation.confidence,
                    boundingBox: CGRect(x: 0.1, y: y, width: 0.8, height: 0.22),
                    emoji: DetectedObject.emojiForLabel(cleanLabel)
                )
            }
        }
    }
    
    // æ¸…ç†æ ‡ç­¾åç§°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    private func cleanLabel(_ label: String) -> String {
        // ç§»é™¤ ImageNet ç±»åˆ«å‰ç¼€ (nå¼€å¤´çš„æ•°å­—)
        var cleaned = label.replacingOccurrences(of: #"^n\d+"#, with: "", options: .regularExpression)
        
        // ç§»é™¤ä¸‹åˆ’çº¿ï¼Œç”¨ç©ºæ ¼æ›¿æ¢
        cleaned = cleaned.replacingOccurrences(of: "_", with: " ")
        
        // åªå–ç¬¬ä¸€ä¸ªè¯ï¼ˆé€šå¸¸æ˜¯ä¸»è¦ç‰©ä½“ï¼‰
        if let firstPart = cleaned.components(separatedBy: ",").first?.trimmingCharacters(in: .whitespaces) {
            cleaned = firstPart
        }
        
        // åªå–æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„è¯
        let words = cleaned.components(separatedBy: " ").filter { !$0.isEmpty }
        if let lastWord = words.last {
            cleaned = lastWord
        }
        
        // é¦–å­—æ¯å¤§å†™
        return cleaned.prefix(1).uppercased() + cleaned.dropFirst().lowercased()
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension ObjectDetectionManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    
    private static var lastProcessTime: Date = Date()
    private static let processingInterval: TimeInterval = 0.5  // æ¯ 0.5 ç§’å¤„ç†ä¸€æ¬¡
    
    func captureOutput(_ output: AVCaptureOutput,
                      didOutput sampleBuffer: CMSampleBuffer,
                      from connection: AVCaptureConnection) {
        
        guard isDetecting,
              let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            return
        }
        
        // èŠ‚æµï¼šæ¯ 0.5 ç§’å¤„ç†ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
        let now = Date()
        guard now.timeIntervalSince(Self.lastProcessTime) >= Self.processingInterval else {
            return
        }
        Self.lastProcessTime = now
        
        // åˆ›å»ºå›¾åƒæ–¹å‘
        let orientation = CGImagePropertyOrientation.right
        
        let handler = VNImageRequestHandler(
            cvPixelBuffer: pixelBuffer,
            orientation: orientation,
            options: [:]
        )
        
        do {
            try handler.perform([classificationRequest])
        } catch {
            print("Failed to perform detection: \(error)")
        }
    }
}

